{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script shows how to use an sklearn estimator (in this case simply knn nearest neighbors) to interpolate point data into continous gridded data within the basin region of the model (the area between the surface from the DEM and the basin elevation surface derived from Abott and Louie). \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my library\n",
    "import points2Rfile\n",
    "from points2Rfile import grid\n",
    "#sklearn\n",
    "from sklearn import linear_model as SK_linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#dask dependencies \n",
    "import dask.array as da\n",
    "from dask_ml.linear_model import LinearRegression as dask_linear_regression\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "#for numpy data type enforcement\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silenceWarnings=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load an hdf5 container with points\n",
    "gridFname = \"renoRemi100m.hdf5\"\n",
    "gridFile = grid.grid(gridFname)\n",
    "#preprare input points to predict accross\n",
    "x=gridFile.x.flatten()\n",
    "y=gridFile.y.flatten()\n",
    "z=gridFile.z.flatten()\n",
    "\n",
    "\n",
    "stackedPoints = da.stack([x,y,z],axis=1)\n",
    "#shape this correctly (has to do with directionality of chunks in memory, note that doing it with the auto option)\n",
    "#is probably pretty inefficient and you may need to think about a better way of doing this\n",
    "print(\"warning this particular rechunking method may be slow!\")\n",
    "#note that I have reduced the chunk size \n",
    "stackedPoints = stackedPoints.rechunk({0: \"auto\", 1: -1},block_size_limit=1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign all input points to an simple nearest neighbor regressor without any weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all of the points availible in this grid file and use them to construct computation\n",
    "points = []\n",
    "for key in gridFile.data[\"points\"].keys():\n",
    "    #construct points object\n",
    "    \n",
    "    points.append(points2Rfile.grid.points(gridFile,key))\n",
    "### Now get all of the points values for each object\n",
    "pointValues = []\n",
    "for point in points:\n",
    "    pointValues.append(point.getPoints())\n",
    "pointValues = da.concatenate(pointValues,axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run k means neighbors regressor for each with n jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silence warnings\n",
    "if silenceWarnings:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "targets = da.stack([pointValues[:,0],pointValues[:,1],pointValues[:,2]],axis=1)\n",
    "#all point values \n",
    "pVp,pVs,pP,pQp,pQs = pointValues[:,3],pointValues[:,4],pointValues[:,5],pointValues[:,6],pointValues[:,7]\n",
    "nNeighbors = 3\n",
    "weight = 'distance' #influence of a point is equal to its inverse distance (so closer ones are more important)\n",
    "modelVp = KNeighborsRegressor(n_neighbors=nNeighbors,n_jobs=-1,weights=weight)\n",
    "modelVp.fit(X=targets,y=pVp)\n",
    "modelVp = ParallelPostFit(estimator=modelVp)\n",
    "vp = modelVp.predict(stackedPoints)\n",
    "print(\"Done with vp computing all of the other ones!\")\n",
    "#coordinate vector for points\n",
    "#vs\n",
    "modelVs = KNeighborsRegressor(n_neighbors=nNeighbors,n_jobs=-1,weights=weight)\n",
    "modelVs.fit(X=targets,y=pVs)\n",
    "modelVs = ParallelPostFit(estimator=modelVs)\n",
    "vs = modelVs.predict(stackedPoints) \n",
    "#p\n",
    "modelp = KNeighborsRegressor(n_neighbors=nNeighbors,n_jobs=-1,weights=weight)\n",
    "modelp.fit(X=targets,y=pP)\n",
    "modelp = ParallelPostFit(estimator=modelp)\n",
    "p = modelp.predict(stackedPoints) \n",
    "#qp\n",
    "modelqp = KNeighborsRegressor(n_neighbors=nNeighbors,n_jobs=-1,weights=weight)\n",
    "modelqp.fit(X=targets,y=pQp)\n",
    "modelqp = ParallelPostFit(estimator=modelqp)\n",
    "qp = modelqp.predict(stackedPoints) \n",
    "#qs\n",
    "modelqs = KNeighborsRegressor(n_neighbors=nNeighbors,n_jobs=-1,weights=weight)\n",
    "modelqs.fit(X=targets,y=pQs)\n",
    "modelqs = ParallelPostFit(estimator=modelqs)\n",
    "qs = modelqs.predict(stackedPoints) \n",
    "#compute and save all to hdf5 file\n",
    "#with parallel_backend('dask'):\n",
    "print(\"preparing to overwrite grid!\")\n",
    "#gridFile.clearGrid()\n",
    "#cast all of this crap as dask arrays\n",
    "\n",
    "gridFile.assignNewGridProperties(vp,vs,p,qp,qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure that everything is cut off below the DEM (since there obviously isnt data in the air above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload the grid file--This shouldnt be necessary but not doing it messes things up sometimes and I have been unable to determine why\n",
    "gridFile = grid.grid(gridFname)\n",
    "gridMaterials = grid.materialProperties(gridFile,mpropsINI=\"mprops.ini\")\n",
    "gridMaterials.cutOffAtDEM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the result as an sw4 compatible rfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import points2Rfile\n",
    "from points2Rfile import grid #maybe dont need this, find out why I would need to any way\n",
    "from points2Rfile import rfile\n",
    "import dask.array as da \n",
    "import numpy as np\n",
    "#top of the rfile\n",
    "maxElevation = -4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridFileName = gridFname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load grid\n",
    "gridFile = grid.grid(gridFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate rfile file io object\n",
    "#name the rFile\n",
    "f = gridFile.fname.split('.')[0] + \".r\"\n",
    "fileObject = open(f,\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write header information\n",
    "rfile.write_hdr(fileObject, magic=1, precision=4, attenuation=1,az=gridFile.mdata[\"AZIMUTH\"],\n",
    "lon0=gridFile.mdata[\"LON0\"], lat0=gridFile.mdata[\"LAT0\"],\n",
    "proj_str=\"+proj=utm +zone=36 +datum=WGS84 +units=m +no_defs\", nb=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write topo header\n",
    "rfile.write_block_hdr(fileObject, gridFile.mdata[\"DX\"],gridFile.mdata[\"DY\"], 0.0, 1,gridFile.mdata[\"NX\"] ,gridFile.mdata[\"NY\"],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data block header\n",
    "rfile.write_block_hdr(fileObject, gridFile.mdata[\"DX\"],gridFile.mdata[\"DZ\"], maxElevation, 5,gridFile.mdata[\"NX\"] ,gridFile.mdata[\"NY\"],gridFile.mdata[\"NZ\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write topo block\n",
    "rfile.write_topo_block(fileObject,gridFile.topo.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write material properties\n",
    "rfile.write_properties(fileObject,gridFile.vp.compute(),5,gridFile.vs.compute(),gridFile.p.compute(),gridFile.qp.compute(),gridFile.qs.compute(),gridFile.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -laht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that the hdf5 file is closed correctly\n",
    "gridFile.data.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
